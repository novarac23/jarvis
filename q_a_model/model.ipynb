{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is base model we have here good enough?\n",
    "# explore improvements\n",
    "  # Maybe different flavor of bert? \n",
    "  # Maybe try a few different models and measure F1 scores?\n",
    "  # Maybe try and fine tune on a different dataset?\n",
    "  # chunk in a way that we tend to get full sentances? Right now a chunk can stop at a random spot in the sentance\n",
    "    \n",
    "  # a few ideas that can help with first three improvemetns are here https://towardsdatascience.com/which-flavor-of-bert-should-you-use-for-your-qa-task-6d6a0897fb24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to fine tune the model (it's fine tuned on squad 2.0)\n",
    "\n",
    "# !python run_squad.py  \\\n",
    "#     --model_type bert   \\\n",
    "#     --model_name_or_path bert-base-uncased  \\\n",
    "#     --output_dir models/bert/ \\\n",
    "#     --data_dir dataset/   \\\n",
    "#     --overwrite_output_dir \\\n",
    "#     --overwrite_cache \\\n",
    "#     --do_train  \\\n",
    "#     --train_file train-v2.0.json   \\\n",
    "#     --version_2_with_negative \\\n",
    "#     --do_lower_case  \\\n",
    "#     --do_eval   \\\n",
    "#     --predict_file dev-v2.0.json   \\\n",
    "#     --per_gpu_train_batch_size 2   \\\n",
    "#     --learning_rate 3e-5   \\\n",
    "#     --num_train_epochs 2.0   \\\n",
    "#     --max_seq_length 384   \\\n",
    "#     --doc_stride 128   \\\n",
    "#     --threads 10   \\\n",
    "#     --save_steps 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wikipedia\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TFAutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/bert\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"models/bert\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who was the first president of the United States?\"\n",
    "\n",
    "q_search = wikipedia.search(question)\n",
    "page = wikipedia.page(q_search[0])\n",
    "context = page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 769), dtype=int32, numpy=\n",
       "array([[  101,  2040,  2001,  1996,  2034,  2343,  1997,  1996,  2142,\n",
       "         2163,  1029,   102,  1996,  2343,  1997,  1996,  2142,  2163,\n",
       "         2003,  1996,  2132,  1997,  2110,  1998,  2132,  1997,  2231,\n",
       "         1997,  1996,  2142,  2163,  1010, 17351,  2700,  2000,  1037,\n",
       "         1018,  1011,  2095,  2744,  2011,  1996,  2111,  2083,  1996,\n",
       "         6092,  2267,  1012,  1996,  2436, 14528,  5260,  1996,  3237,\n",
       "         3589,  1997,  1996,  2976,  2231,  1998,  2003,  1996,  3474,\n",
       "         1011,  1999,  1011,  2708,  1997,  1996,  2142,  2163,  4273,\n",
       "         2749,  1012,  2144,  1996,  2436,  2001,  2511,  1999, 13739,\n",
       "         1010,  4008,  2273,  2031,  2366,  2004,  2343,  1012,  1996,\n",
       "         2034,  1010,  2577,  2899,  1010,  2180,  1037, 13604,  3789,\n",
       "         1997,  1996,  6092,  2267,  1012, 25242,  6044,  2366,  2048,\n",
       "         2512,  1011,  5486,  3408,  1999,  2436,  1006,  1996,  2069,\n",
       "         2343,  2000,  2031,  2589,  2061,  1007,  1998,  2003,  3568,\n",
       "         8897,  2004,  1996, 13816,  1998, 13386,  2343,  1997,  1996,\n",
       "         2142,  2163,  1025,  1996, 24634,  1998,  2783,  2343,  2003,\n",
       "         6221,  8398,  1006,  2144,  2254,  2322,  1010,  2418,  1007,\n",
       "         1012,  2045,  2024,  2747,  2176,  2542,  2280, 11274,  1012,\n",
       "         1996,  2087,  3522,  2280,  2343,  2000,  3280,  2001,  2577,\n",
       "         1044,  1012,  1059,  1012,  5747,  1010,  2006,  2281,  2382,\n",
       "         1010,  2760,  1012,  1996,  8798,  1997,  2520,  2888,  6676,\n",
       "         1010,  2040,  2351,  2861,  2420,  2044,  2635,  2436,  1999,\n",
       "         9840,  1010,  2001,  1996, 20047,  1999,  2137,  2381,  1012,\n",
       "         5951,  1040,  1012,  8573,  2366,  1996,  6493,  1010,  2058,\n",
       "         4376,  2086,  1010,  2077,  5996,  2220,  1999,  2010,  2959,\n",
       "         2744,  1999,  3386,  1012,  2002,  2003,  1996,  2069,  1057,\n",
       "         1012,  1055,  1012,  2343,  2000,  2031,  2366,  2062,  2084,\n",
       "         2048,  3408,  1012,  2144,  1996, 27369,  1997,  1996,  3174,\n",
       "         1011,  2117,  7450,  2000,  1996,  2142,  2163,  4552,  1999,\n",
       "         4131,  1010,  2053,  2711,  2089,  2022,  2700,  2343,  2062,\n",
       "         2084,  3807,  1998,  2053,  2028,  2040,  2038,  2366,  2062,\n",
       "         2084,  2048,  2086,  1997,  1037,  2744,  2000,  2029,  2619,\n",
       "         2842,  2001,  2700,  2089,  2022,  2700,  2062,  2084,  2320,\n",
       "         1012,  1997,  2216,  2040,  2031,  2366,  2004,  1996,  3842,\n",
       "         1005,  1055,  2343,  1010,  2176,  2351,  1999,  2436,  1997,\n",
       "         3019,  5320,  1006,  2520,  2888,  6676,  1010, 19474,  4202,\n",
       "         1010,  6031,  1043,  1012, 15456,  1010,  1998,  5951,  1040,\n",
       "         1012,  8573,  1007,  1010,  2176,  2020, 16370,  1006,  8181,\n",
       "         5367,  1010,  2508,  1037,  1012, 20170,  1010,  2520, 22121,\n",
       "         1998,  2198,  1042,  1012,  5817,  1007,  1010,  1998,  2028,\n",
       "         5295,  1006,  2957, 11296,  1010,  5307, 17727,  5243, 22729,\n",
       "         1007,  1012,  2198,  7482,  2001,  1996,  2034,  3580,  2343,\n",
       "         2000,  7868,  1996,  8798,  2076,  1037,  4883,  2744,  1010,\n",
       "         1998,  2275,  1996, 20056,  2008,  1037,  3580,  2343,  2040,\n",
       "         2515,  2061,  4150,  1996,  3929, 12285,  2343,  2007,  2010,\n",
       "         8798,  1010,  2004,  4941,  2000,  1037, 17600,  2343,  1012,\n",
       "         1996,  3174,  1011,  3587,  7450,  2000,  1996,  4552,  2404,\n",
       "         7482,  1005,  1055, 20056,  2046,  2375,  1999,  3476,  1012,\n",
       "         2009,  2036,  2511,  1037,  7337,  2011,  2029,  2019, 26721,\n",
       "         1011,  2744, 15619,  1999,  1996,  3580,  8798,  2071,  2022,\n",
       "         3561,  1012,  2957, 11296,  2001,  1996,  2034,  2343,  2000,\n",
       "         6039,  1037, 15619,  2104,  2023,  9347,  2043,  2002,  3479,\n",
       "         9659,  4811,  2005,  1996,  2436,  2206, 11867,  9711, 12943,\n",
       "         2638,  2860,  1005,  1055,  8172,  1999,  3381,  1012,  1996,\n",
       "         2206,  2095,  1010,  4811,  2150,  1996,  2117,  2000,  2079,\n",
       "         2061,  2043,  2002,  4900,  5912, 16696,  2000,  9510,  2032,\n",
       "         2044,  2002, 16222, 19082,  2000,  1996,  8798,  1012,  2004,\n",
       "         2053,  7337,  5839,  2005,  8110,  2019, 26721,  1011,  2744,\n",
       "        15619,  1999,  1996,  3580,  8798,  2077,  3476,  1010,  1996,\n",
       "         2436,  2001,  2187, 10030,  2127,  3561,  2083,  1996,  2279,\n",
       "        13831,  4883,  2602,  1998,  4745, 17331,  1012,  2802,  2087,\n",
       "         1997,  2049,  2381,  1010,  2137,  4331,  2038,  2042,  6817,\n",
       "         2011,  2576,  4243,  1012,  1996,  4552,  2003,  4333,  2006,\n",
       "         1996,  3277,  1997,  2576,  4243,  1010,  1998,  2012,  1996,\n",
       "         2051,  2009,  2234,  2046,  2486,  1999, 13739,  1010,  2045,\n",
       "         2020,  2053,  4243,  1012,  2574,  2044,  1996,  3083,  3519,\n",
       "        19596,  1010, 13815,  2211,  8320,  2075,  2105,  7444,  2899,\n",
       "         3447,  4584,  1010,  2107,  2004,  3656,  5226,  1998,  2726,\n",
       "         7625,  1012,  6551,  4986,  2055,  1996,  3977,  1997,  2576,\n",
       "         4243,  2000,  6033,  1996, 13072,  8499,  3173,  1996,  3842,\n",
       "         2362,  1010,  2899,  2815, 14477, 26989,  6632,  3064,  2007,\n",
       "         2151,  2576, 10233,  2030,  2283,  2802,  2010,  2809,  1011,\n",
       "         2095,  8798,  1012,  2002,  2001,  1010,  1998,  3464,  1010,\n",
       "         1996,  2069,  1057,  1012,  1055,  1012,  2343,  2196,  6989,\n",
       "         2007,  1037,  2576,  2283,  1012,  1027,  1027, 11274,  1027,\n",
       "         1027,  1027,  1027,  4745,  2270,  2436,  1027,  1027,  2093,\n",
       "         2280, 11274,  2218,  2178,  1057,  1012,  1055,  1012,  2976,\n",
       "         2436,  2044,  3529,  2004,  2343,  1012,  1027,  1027,  2156,\n",
       "         2036,  1027,  1027,  3772,  2343,  1997,  1996,  2142,  2163,\n",
       "         4889, 11397,  1997,  1996,  2142,  2163,  2862,  1997, 11274,\n",
       "         1997,  1996,  6803,  3519,  2862,  1997,  3580, 11274,  1997,\n",
       "         1996,  2142,  2163,  1027,  1027,  3964,  1027,  1027,  1027,\n",
       "         1027,  7604,  1027,  1027,  1027,  1027,  6327,  6971,  1027,\n",
       "         1027,  2317,  4580,  1012, 18079,  1024,  1996, 11274,  5292,\n",
       "        24997,  8602,  2415,  1064,  4883,  4105,  2913,  2012,  2882,\n",
       "         3028,  2110,  2118,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 769), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 769), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "      dtype=int32)>}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answering wikipedia questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mask = inputs[\"token_type_ids\"] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tensor = tf.boolean_mask(inputs[\"input_ids\"], question_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12,), dtype=int32, numpy=\n",
       "array([ 101, 2040, 2001, 1996, 2034, 2343, 1997, 1996, 2142, 2163, 1029,\n",
       "        102], dtype=int32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = max_length - len(question_tensor) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_input = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in inputs.items():\n",
    "    question = tf.boolean_mask(v, question_mask)\n",
    "    context =  tf.boolean_mask(v, ~question_mask)\n",
    "    chunks = list(get_chunks(context, chunk_size))\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i not in chunked_input:\n",
    "            chunked_input[i] = {}\n",
    "            \n",
    "        entire_input = tf.concat([question, chunk], axis=0)\n",
    "        \n",
    "        if i != len(chunks) - 1:\n",
    "            if k == 'input_ids':\n",
    "                entire_input = tf.concat([entire_input, tf.constant([102])], axis=0)\n",
    "            else:\n",
    "                entire_input = tf.concat([entire_input, tf.constant([1])], axis=0)\n",
    "        \n",
    "        chunked_input[i][k] = tf.reshape(entire_input, [1, entire_input.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[  101,  2040,  2001,  1996,  2034,  2343,  1997,  1996,  2142,\n",
       "          2163,  1029,   102,  1996,  2343,  1997,  1996,  2142,  2163,\n",
       "          2003,  1996,  2132,  1997,  2110,  1998,  2132,  1997,  2231,\n",
       "          1997,  1996,  2142,  2163,  1010, 17351,  2700,  2000,  1037,\n",
       "          1018,  1011,  2095,  2744,  2011,  1996,  2111,  2083,  1996,\n",
       "          6092,  2267,  1012,  1996,  2436, 14528,  5260,  1996,  3237,\n",
       "          3589,  1997,  1996,  2976,  2231,  1998,  2003,  1996,  3474,\n",
       "          1011,  1999,  1011,  2708,  1997,  1996,  2142,  2163,  4273,\n",
       "          2749,  1012,  2144,  1996,  2436,  2001,  2511,  1999, 13739,\n",
       "          1010,  4008,  2273,  2031,  2366,  2004,  2343,  1012,  1996,\n",
       "          2034,  1010,  2577,  2899,  1010,  2180,  1037, 13604,  3789,\n",
       "          1997,  1996,  6092,  2267,  1012, 25242,  6044,  2366,  2048,\n",
       "          2512,  1011,  5486,  3408,  1999,  2436,  1006,  1996,  2069,\n",
       "          2343,  2000,  2031,  2589,  2061,  1007,  1998,  2003,  3568,\n",
       "          8897,  2004,  1996, 13816,  1998, 13386,  2343,  1997,  1996,\n",
       "          2142,  2163,  1025,  1996, 24634,  1998,  2783,  2343,  2003,\n",
       "          6221,  8398,  1006,  2144,  2254,  2322,  1010,  2418,  1007,\n",
       "          1012,  2045,  2024,  2747,  2176,  2542,  2280, 11274,  1012,\n",
       "          1996,  2087,  3522,  2280,  2343,  2000,  3280,  2001,  2577,\n",
       "          1044,  1012,  1059,  1012,  5747,  1010,  2006,  2281,  2382,\n",
       "          1010,  2760,  1012,  1996,  8798,  1997,  2520,  2888,  6676,\n",
       "          1010,  2040,  2351,  2861,  2420,  2044,  2635,  2436,  1999,\n",
       "          9840,  1010,  2001,  1996, 20047,  1999,  2137,  2381,  1012,\n",
       "          5951,  1040,  1012,  8573,  2366,  1996,  6493,  1010,  2058,\n",
       "          4376,  2086,  1010,  2077,  5996,  2220,  1999,  2010,  2959,\n",
       "          2744,  1999,  3386,  1012,  2002,  2003,  1996,  2069,  1057,\n",
       "          1012,  1055,  1012,  2343,  2000,  2031,  2366,  2062,  2084,\n",
       "          2048,  3408,  1012,  2144,  1996, 27369,  1997,  1996,  3174,\n",
       "          1011,  2117,  7450,  2000,  1996,  2142,  2163,  4552,  1999,\n",
       "          4131,  1010,  2053,  2711,  2089,  2022,  2700,  2343,  2062,\n",
       "          2084,  3807,  1998,  2053,  2028,  2040,  2038,  2366,  2062,\n",
       "          2084,  2048,  2086,  1997,  1037,  2744,  2000,  2029,  2619,\n",
       "          2842,  2001,  2700,  2089,  2022,  2700,  2062,  2084,  2320,\n",
       "          1012,  1997,  2216,  2040,  2031,  2366,  2004,  1996,  3842,\n",
       "          1005,  1055,  2343,  1010,  2176,  2351,  1999,  2436,  1997,\n",
       "          3019,  5320,  1006,  2520,  2888,  6676,  1010, 19474,  4202,\n",
       "          1010,  6031,  1043,  1012, 15456,  1010,  1998,  5951,  1040,\n",
       "          1012,  8573,  1007,  1010,  2176,  2020, 16370,  1006,  8181,\n",
       "          5367,  1010,  2508,  1037,  1012, 20170,  1010,  2520, 22121,\n",
       "          1998,  2198,  1042,  1012,  5817,  1007,  1010,  1998,  2028,\n",
       "          5295,  1006,  2957, 11296,  1010,  5307, 17727,  5243, 22729,\n",
       "          1007,  1012,  2198,  7482,  2001,  1996,  2034,  3580,  2343,\n",
       "          2000,  7868,  1996,  8798,  2076,  1037,  4883,  2744,  1010,\n",
       "          1998,  2275,  1996, 20056,  2008,  1037,  3580,  2343,  2040,\n",
       "          2515,  2061,  4150,  1996,  3929, 12285,  2343,  2007,  2010,\n",
       "          8798,  1010,  2004,  4941,  2000,  1037, 17600,  2343,  1012,\n",
       "          1996,  3174,  1011,  3587,  7450,  2000,  1996,  4552,  2404,\n",
       "          7482,  1005,  1055, 20056,  2046,  2375,  1999,  3476,  1012,\n",
       "          2009,  2036,  2511,  1037,  7337,  2011,  2029,  2019, 26721,\n",
       "          1011,  2744, 15619,  1999,  1996,  3580,  8798,  2071,  2022,\n",
       "          3561,  1012,  2957, 11296,  2001,  1996,  2034,  2343,  2000,\n",
       "          6039,  1037, 15619,  2104,  2023,  9347,  2043,  2002,  3479,\n",
       "          9659,  4811,  2005,  1996,  2436,  2206, 11867,  9711, 12943,\n",
       "          2638,  2860,  1005,  1055,  8172,  1999,  3381,  1012,  1996,\n",
       "          2206,  2095,  1010,  4811,  2150,  1996,  2117,  2000,  2079,\n",
       "          2061,  2043,  2002,  4900,  5912, 16696,  2000,  9510,  2032,\n",
       "          2044,  2002, 16222, 19082,  2000,  1996,  8798,   102]],\n",
       "       dtype=int32)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]], dtype=int32)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = ''\n",
    "\n",
    "for k, chunk in chunked_input.items():\n",
    "    start_scores, end_scores = model(chunk)\n",
    "\n",
    "    answer_start = tf.argmax(start_scores, axis=1).numpy()[0]\n",
    "    answer_end = (tf.argmax(end_scores, axis=1) + 1).numpy()[0]\n",
    "    answ = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(chunk['input_ids'][0][answer_start:answer_end]))\n",
    "    \n",
    "    if answ != '[CLS]':\n",
    "        answer += answ + \" / \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'george washington / '"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
